{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations done before\n",
    "    \n",
    "    pip3 install pytorch-pretrained-bert\n",
    "    pip3 install pytorch-nlp\n",
    "    pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load first query and paragraphs from HotPotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath('../data/hotpot_train_v1.1.json')) as json_file:\n",
    "    hotpot_train = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Which magazine was started first Arthur's Magazine or First for Women?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = hotpot_train[0]['question']\n",
    "paragraphs = hotpot_train[0]['context']\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Radio City (Indian radio station)',\n",
       " [\"Radio City is India's first private FM radio station and was started on 3 July 2001.\",\n",
       "  ' It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).',\n",
       "  ' It plays Hindi, English and regional songs.',\n",
       "  ' It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.',\n",
       "  ' Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.',\n",
       "  ' The Radio station currently plays a mix of Hindi and Regional music.',\n",
       "  ' Abraham Thomas is the CEO of the company.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                 output_hidden_states=True,\n",
    "                                 output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was not a puppeteer. He died too early. [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   101,  2040,  2001,  3958, 27227,  1029,   102,  3958, 27227,\n",
       "          2001,  2025,  1037, 13997, 11510,  1012,  2002,  2351,  2205,  2220,\n",
       "          1012,   102,   102]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids must be a list of lists\n",
    "input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hidden_states, all_attentions = model(input_ids)[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 23, 23])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 12 \"attentions\" - one for each \"encoder layer\"; each of these layers\n",
    "# has 12 attention heads. Each attention head has a dimention n x n,\n",
    "# where n is the number of words in the sentence.\n",
    "all_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 768])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Radio City is India's first private FM radio station and was started on 3 July 2001. [SEP] It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003). [SEP] It plays Hindi, English and regional songs. [SEP] It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007. [SEP] Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. [SEP] The Radio station currently plays a mix of Hindi and Regional music. [SEP] Abraham Thomas is the CEO of the company. [SEP]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add [CLS] and [SEP] to paragraph and join sentences for each paragraph together\n",
    "test_paragraph = \"[CLS] \" + (\" [SEP]\").join(paragraphs[0][1]) + \" [SEP]\"\n",
    "test_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'radio', 'city', 'is', 'india', \"'\", 's', 'first', 'private', 'fm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_paragraph = tokenizer.tokenize(test_paragraph)\n",
    "tokenized_paragraph[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'jim',\n",
       " 'henson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text,\n",
    "           tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "           model=BertModel.from_pretrained('bert-base-uncased',\n",
    "                                 output_hidden_states=True,\n",
    "                                 output_attentions=True)):\n",
    "    ''' TODO: document\n",
    "    '''\n",
    "    \n",
    "    input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "    all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "    \n",
    "    # This is the embedding of the [CLS] token.\n",
    "    # [-1] is the last hidden state (list of sentences)\n",
    "    # first [0] - first (and only) sentence\n",
    "    # second [0] - first ([CLS]) token of the sentence\n",
    "    return all_hidden_states[-1][0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph selector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParagraphSelector(torch.nn.Module):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size=768, output_size=1):\n",
    "        self.linear  = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "        output = self.linear(embedding)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data(hotpot_train):\n",
    "    '''\n",
    "    Make a dataframe with training data for selecting relevant paragraphs\n",
    "    Each entry in the dataframe has three columns:\n",
    "        1. Query - the question\n",
    "        2. Paragraphs - the paragraphs\n",
    "        3. Label - 0 (unrelated) or 1 (related)\n",
    "    '''\n",
    "    for item in hotpot_train[:10]:\n",
    "        query = item['question']\n",
    "        paragraphs = item['context']\n",
    "        supporting_facts = [i[0] for i in item['supporting_facts']]\n",
    "        \n",
    "        labels = []\n",
    "        datapoints = []\n",
    "        for para in paragraphs:\n",
    "            labels.append(int(para[0] in supporting_facts))\n",
    "            datapoints.append(\"[CLS] \" + query + \" [SEP] \" + (\"\").join(para[1]) + \" [SEP]\")\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'id': range(len(labels)),\n",
    "            'label': labels,\n",
    "            'text': datapoints\n",
    "        })\n",
    "        return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_training_data(hotpot_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[CLS] Which magazine was started first Arthur'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                               text\n",
       "0   0      0  [CLS] Which magazine was started first Arthur'...\n",
       "1   1      0  [CLS] Which magazine was started first Arthur'...\n",
       "2   2      0  [CLS] Which magazine was started first Arthur'...\n",
       "3   3      0  [CLS] Which magazine was started first Arthur'...\n",
       "4   4      0  [CLS] Which magazine was started first Arthur'...\n",
       "5   5      1  [CLS] Which magazine was started first Arthur'...\n",
       "6   6      0  [CLS] Which magazine was started first Arthur'...\n",
       "7   7      1  [CLS] Which magazine was started first Arthur'...\n",
       "8   8      0  [CLS] Which magazine was started first Arthur'...\n",
       "9   9      0  [CLS] Which magazine was started first Arthur'..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Which magazine was started first Arthur's Magazine or First for Women? [SEP] Radio City is India's first private FM radio station and was started on 3 July 2001. It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003). It plays Hindi, English and regional songs. It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007. Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. The Radio station currently plays a mix of Hindi and Regional music. Abraham Thomas is the CEO of the company. [SEP]\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "labels = []\n",
    "for point in data.loc[:,['label','text']].values:\n",
    "#     print(point)\n",
    "    train_data.append(encode(point[1]))\n",
    "    labels.append(point[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, labels, net, epochs, learning_rate=0.0001):\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(lr=learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Iterate over the epochs\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d/%d' % (epoch + 1, epochs))\n",
    "        for inputs, label in zip(train_data, labels):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_data):\n",
    "# set the model into evaluation mode and turn off autograd to save memory\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_data):\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network: %d %%' % (100 * correct / total))\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
