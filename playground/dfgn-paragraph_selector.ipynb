{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations done before\n",
    "    \n",
    "    pip3 install pytorch-pretrained-bert\n",
    "    pip3 install pytorch-nlp\n",
    "    pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "from utils import HotPotDataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model=BertModel.from_pretrained('bert-base-uncased',\n",
    "                                 output_hidden_states=True,\n",
    "                                 output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text,\n",
    "           tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "           model=BertModel.from_pretrained('bert-base-uncased',\n",
    "                                 output_hidden_states=True,\n",
    "                                 output_attentions=True)):\n",
    "    ''' TODO: document\n",
    "    '''\n",
    "    \n",
    "    input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "    all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "    \n",
    "    # This is the embedding of the [CLS] token.\n",
    "    # [-1] is the last hidden state (list of sentences)\n",
    "    # first [0] - first (and only) sentence\n",
    "    # second [0] - first ([CLS]) token of the sentence\n",
    "    return all_hidden_states[-1][0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph selector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParagraphSelector_non_final(torch.nn.Module):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size=768, output_size=1):\n",
    "        self.linear  = torch.nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "        output = self.linear(embedding)\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data_from_filedata(hotpot_train):\n",
    "    '''\n",
    "    Make a dataframe with training data for selecting relevant paragraphs\n",
    "    Each entry in the dataframe has three columns:\n",
    "        1. Query - the question\n",
    "        2. Paragraphs - the paragraphs\n",
    "        3. Label - 0 (unrelated) or 1 (related)\n",
    "    '''\n",
    "    for item in hotpot_train[:10]:\n",
    "        query = item['question']\n",
    "        paragraphs = item['context']\n",
    "        supporting_facts = [i[0] for i in item['supporting_facts']]\n",
    "        \n",
    "        labels = []\n",
    "        datapoints = []\n",
    "        for para in paragraphs:\n",
    "            labels.append(int(para[0] in supporting_facts))\n",
    "            datapoints.append(\"[CLS] \" + query + \" [SEP] \" + (\"\").join(para[1]) + \" [SEP]\")\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'id': range(len(labels)),\n",
    "            'label': labels,\n",
    "            'text': datapoints\n",
    "        })\n",
    "        return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements the Paragraph Selector from the paper, Section 3.1\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class ParagraphSelector():\n",
    "    \"\"\"\n",
    "    TODO: write docstring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokenizer=None,\n",
    "                 model=None):\n",
    "        \"\"\"\n",
    "        TODO: write docstring\n",
    "        \"\"\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') if not tokenizer else tokenizer\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                 output_hidden_states=True,\n",
    "                                 output_attentions=True) if not model else model\n",
    "        \n",
    "        class ParagraphSelectorNet(torch.nn.Module):\n",
    "            \"\"\"\n",
    "            TODO: write docstring\n",
    "            \"\"\"\n",
    "            def __init__(self, input_size=768, output_size=1):\n",
    "                super(ParagraphSelectorNet, self).__init__()\n",
    "                self.linear  = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "            def forward(self, embedding):\n",
    "                output = self.linear(embedding)\n",
    "                output = torch.sigmoid(output)\n",
    "\n",
    "                return output \n",
    "            \n",
    "        self.net = ParagraphSelectorNet()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        ''' TODO: document\n",
    "        '''\n",
    "\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(text)])\n",
    "        all_hidden_states, all_attentions = self.model(input_ids)[-2:]\n",
    "\n",
    "        # This is the embedding of the [CLS] token.\n",
    "        # [-1] is the last hidden state (list of sentences)\n",
    "        # first [0] - first (and only) sentence\n",
    "        # second [0] - first ([CLS]) token of the sentence\n",
    "        return all_hidden_states[-1][0][0]\n",
    "\n",
    "    def train(self, train_data, labels, epochs, learning_rate=0.0001):\n",
    "        \"\"\"\n",
    "        TODO: write docstring\n",
    "        \"\"\"\n",
    "        # Use Binary Cross Entropy as a loss function instead of MSE\n",
    "        # There are papers on why MSE is bad for classification\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "\n",
    "        losses = []\n",
    "        \n",
    "        # Set the network into train mode\n",
    "        self.net.train()\n",
    "\n",
    "        print(\"Training...\")\n",
    "\n",
    "        # Iterate over the epochs\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %d/%d' % (epoch + 1, epochs))\n",
    "            for inputs, label in zip(train_data, labels):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.net(inputs)\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward(retain_graph=True)\n",
    "                losses.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict(self, p):\n",
    "        \"\"\"\n",
    "        TODO: write docstring\n",
    "        \"\"\"\n",
    "        self.net.eval()\n",
    "        score = self.net(p)\n",
    "        return score\n",
    "\n",
    "    def test(self, test_data, labels):\n",
    "        \"\"\"\n",
    "        TODO: write docstring\n",
    "        \"\"\"\n",
    "        # set the model into evaluation mode and turn off autograd to save memory\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_data):\n",
    "                text, labels = data\n",
    "                outputs = net(text)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print('Accuracy of the network: %d %%' % (100 * correct / total))\n",
    "        return correct / total\n",
    "    \n",
    "    def make_context(self, paragraphs, query, threshold):\n",
    "        \"\"\"\n",
    "        TODO: write docstring\n",
    "        \n",
    "        Parameters: paragraphs - [[p1_title, [p1_s1, p1_s2 ...]],\n",
    "                                  [p2_title, [p2_s1, p2_s2, ...]],\n",
    "                                   ...]\n",
    "                    query - the query as a string\n",
    "                    threshold - a float between zero and one;\n",
    "                                paragraphs that get a score above the\n",
    "                                threshold, become part of the context\n",
    "        Output: context: [[p1_title, [p1_s1, p1_s2 ...]],\n",
    "                          [p2_title, [p2_s1, p2_s2, ...]],\n",
    "                           ...]\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        for p in paragraphs:\n",
    "            # p[0] is the paragraph title, p[1] is the list of sentences in the paragraph\n",
    "            encoded_p = self.encode(\"[CLS] \" + query + \" [SEP] \" + (\"\").join(p[1]) + \" [SEP]\")\n",
    "            score = self.predict(encoded_p)\n",
    "            if score > threshold:\n",
    "                context.append(p)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = HotPotDataHandler(parent_dir + \"/data/hotpot_train_v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dh.data_for_paragraph_selector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data(data,\n",
    "                       tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "                       model=BertModel.from_pretrained('bert-base-uncased',\n",
    "                                 output_hidden_states=True,\n",
    "                                 output_attentions=True)):\n",
    "    '''\n",
    "    Make a dataframe with training data for selecting relevant paragraphs\n",
    "    Each entry in the dataframe has three columns:\n",
    "        1. Query - the question\n",
    "        2. Paragraphs - the paragraphs\n",
    "        3. Label - 0 (unrelated) or 1 (related)\n",
    "    '''\n",
    "    labels = []\n",
    "    datapoints = []\n",
    "    for point in data:        \n",
    "        for para in point[2]:\n",
    "            labels.append(torch.Tensor([int(para[0] in point[0])])) # Label 1: if paragraph title is in supporting facts, otherwise 0\n",
    "            encoded_point = encode(\"[CLS] \" + point[1] + \" [SEP] \" + (\"\").join(para[1]) + \" [SEP]\", tokenizer, model)\n",
    "            datapoints.append(encoded_point)\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        'id': range(len(labels)),\n",
    "        'label': labels,\n",
    "        'text': datapoints\n",
    "    })\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = make_training_data(data[:2])\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data[[\"id\", \"text\"]], training_data[\"label\"], test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = ParagraphSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "losses = ps.train(X_train[\"text\"], y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(-0.7114, grad_fn=&lt;SelectBackward&gt;), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[tensor(-0.0008, grad_fn=&lt;SelectBackward&gt;), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[tensor(-0.4962, grad_fn=&lt;SelectBackward&gt;), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[tensor(-0.0445, grad_fn=&lt;SelectBackward&gt;), te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text\n",
       "0    0  [tensor(-0.7114, grad_fn=<SelectBackward>), te...\n",
       "17  17  [tensor(-0.0008, grad_fn=<SelectBackward>), te...\n",
       "15  15  [tensor(-0.4962, grad_fn=<SelectBackward>), te...\n",
       "1    1  [tensor(-0.0445, grad_fn=<SelectBackward>), te..."
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3437], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.3113], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.2753], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.3116], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "for index, row in X_test.iterrows():\n",
    "    print(ps.predict(X_test[\"text\"][index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ps.make_context(data[2][2], data[2][1], 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Marge Simpson',\n",
       "  ['Marjorie Jacqueline \"Marge\" Simpson (n√©e Bouvier) is a fictional character in the American animated sitcom \"The Simpsons\" and part of the eponymous family.',\n",
       "   ' She is voiced by Julie Kavner and first appeared on television in \"The Tracey Ullman Show\" short \"Good Night\" on April 19, 1987.',\n",
       "   \" Marge was created and designed by cartoonist Matt Groening while he was waiting in the lobby of James L. Brooks' office.\",\n",
       "   ' Groening had been called to pitch a series of shorts based on \"Life in Hell\" but instead decided to create a new set of characters.',\n",
       "   ' He named the character after his mother Margaret Groening.',\n",
       "   ' After appearing on \"The Tracey Ullman Show\" for three seasons, the Simpson family received their own series on Fox, which debuted December 17, 1989.']],\n",
       " ['Allie Goertz',\n",
       "  ['Allison Beth \"Allie\" Goertz (born March 2, 1991) is an American musician.',\n",
       "   ' Goertz is known for her satirical songs based on various pop culture topics.',\n",
       "   ' Her videos are posted on YouTube under the name of Cossbysweater.',\n",
       "   ' Subjects of her songs have included the film \"The Room\", the character Milhouse from the television show \"The Simpsons\", and the game Dungeons & Dragons.',\n",
       "   ' Her style has been compared to that of Bo Burnham.',\n",
       "   ' In December 2015, Goertz released a concept album based on the Adult Swim series \"Rick and Morty\", \"Sad Dance Songs\", with the album\\'s cover emulating the animation and logo of the series.',\n",
       "   ' The album was made possible through Kickstarter.',\n",
       "   \" She is co-host of Everything's Coming Up Podcast, a Simpsons-focused podcast along with Julia Prescott.\"]],\n",
       " ['Milhouse Van Houten',\n",
       "  ['Milhouse Mussolini van Houten is a fictional character featured in the animated television series \"The Simpsons\", voiced by Pamela Hayden, and created by Matt Groening who named the character after President Richard Nixon\\'s middle name.',\n",
       "   ' Later in the series, it is revealed that Milhouse\\'s middle name is \"Mussolini.\"']]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
