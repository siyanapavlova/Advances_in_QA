# this config is for running the DFGN pipeline on the Jones machine.
# Here are some hints towards parameter names:
# 'ps' = paragraph selector
# 'fb' = facebook (jk lol it's actually 'fusion block')

# absolute path to the directory in which the model outputs are (model, times, losses etc.)
model_abs_dir       '/local/simonp/AQA/data_in_QA/models/'

# FOR TRAINING ON THE WHOLE DATASET
data_abs_path       '/local/simonp/data/hotpot_train_v1.1.json'
dev_data_abs_path   '/local/simonp/data/hotpot_dev_distractor_v1.json'

# FOR TRAINING ON SMALL AMOUNTS OF DATA
# leave this unspecified if training on the whole data set
#pickled_train_data
#pickled_dev_data


# PARAGRAPH SELECTOR
# for loading a previously trained paragraph selector model
ps_model_abs_path   '/local/simonp/AQA/data_in_QA/models/PS_final_2020-05-05/' #TODO: set this correctly
ps_threshold        0.1

# ENTITY GRAPH
use_gpu_for_ner      True

# ENCODER
# used throughout DFGN
bert_model_path     'bert-base-uncased'
text_length     250
emb_size  300

# FUSION BLOCK
fb_passes           2
fb_dropout          0.5

# PREDICTOR
predictor_dropout         0.3


# HYPERPARAMETERS
try_training_on_gpu     True

# number of questions for training/evaluation
training_dataset_size         100
# this should be 0.01 for the big run (in order to have 1000 questions for eval. during training)
percent_for_eval_during_training      0.10
shuffle_seed    42

# Coefficients to weight the loss values (section 3.5)
lambda_s            0.5
lambda_t            0.5


epochs              1
# number of questions per batch
batch_size          16
learning_rate       1e-4
eval_interval       20

