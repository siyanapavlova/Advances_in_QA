# this config is for running the DFGN pipeline on the Jones machine.
# Here are some hints towards parameter names:
# 'ps' = paragraph selector
# 'fb' = facebook (jk lol it's actually 'fusion block')

# absolute path to the directory in which the model outputs are (model, times, losses etc.)
model_abs_dir       '/local/simonp/AQA/data_in_QA/models/'

# FOR TRAINING ON THE WHOLE DATASET
data_abs_path       '/local/simonp/data/hotpot_train_v1.1.json'
dev_data_abs_path   '/local/simonp/data/hotpot_dev_distractor_v1.json'

# FOR TRAINING ON SMALL AMOUNTS OF DATA
# leave this unspecified if training on the whole data set
#pickled_train_data   '/local/simonp/data/pickled/train'
#pickled_dev_data     '/local/simonp/data/pickled/dev'


# PARAGRAPH SELECTOR
# for loading a previously trained paragraph selector model
ps_model_abs_path   '/local/simonp/AQA/data_in_QA/models/PS_final_2020-05-05/'
ps_threshold        0.1

# ENTITY GRAPH
use_gpu_for_ner      True

# ENCODER
# used throughout DFGN
bert_model_path     'bert-base-uncased'
text_length         250
emb_size            300

# FUSION BLOCK
fb_passes           2
fb_dropout          0.5

# PREDICTOR
predictor_dropout   0.3
# Coefficients to weight the loss values (section 3.5)
lambda_s            0.5
lambda_t            0.5


# HYPERPARAMETERS
try_training_on_gpu     True
# for training on jones-5, use one of [0,1,2,3]
gpu_number              3

# number of questions for training/evaluation
training_dataset_size         40000
# this should be 0.01 for the big run (in order to have 1000 questions for eval. during training)
percent_for_eval_during_training      0.01
shuffle_seed    42



epochs              1
# number of questions per batch (max. 10 on jones-5)
batch_size          10
learning_rate       1e-5
# THIS COUNTS BATCHES
# (for 10 eval rounds, set this to (training_dataset_size/batch_size)/10 )
eval_interval       300
eval_data_dump_dir   '/local/simonp/AQA/data_in_QA/tmp/small_lr/'

verbose_evaluation  True

